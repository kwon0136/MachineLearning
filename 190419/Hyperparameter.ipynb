{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hyperparameter.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kwon0136/MachineLearning/blob/master/190419/Hyperparameter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "gBXPROaeS8Pb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Hyper Parameter란\n",
        "신경망 학습을 통해서 튜닝 또는 최적화 해야하는 주변수가 아니라,\n",
        "학습 진도율이나 일반화 변수처럼,\n",
        "사람들이 **선험적 지식으로 설정을 하거나 또는 외부 모델 메커니즘을 통해 자동으로 설정이 되는 변수**를 말한다.\n",
        "\n",
        "사례\n",
        "\n",
        "1. **Learning Rate**\n",
        "학습진도율은 “gradient”의 방향으로 얼마나 빠르게 이동을 할 것인지를 결정한다. 학습진도율이 너무 작으면 학습의 속도가 너무 느리게 되고, 반대로 너무 크면 학습이 안되고 진동할 수 있다. 학습 진도율도 학습 대상이나 망에 따라 적절히 조절해야한다.\n",
        "\n",
        "2. **Cost function**\n",
        "일반적인 최소자승법을 사용할수 도 있고, cross-entropy 함수를 사용할 수도 있다.\n",
        "\n",
        "3. **Regularization parameter**\n",
        "overfitting 문제를 피하기 위해 L1이나 L2 regularization 방법을 사용할 수도 있고 거기서 사용하는 일반화 변수는 weight decay의 속도를 조절하기 위한 용도로 사용할 수가 있다.\n",
        "\n",
        "4. **Mini-batch 크기**\n",
        "Mini-batch크기가 큰 경우 병렬연산 구조를 사용할 때 효과적일 수 있으며, 크기가 작으면 더 많은 update를 할 수가 있다.\n",
        "\n",
        "5. ** Training 반복횟수**\n",
        "학습의 조기 종료를 결정하는 변수가 된다.\n",
        "\n",
        "6. ** Hidden Unit의 개수**\n",
        "Hidden Layer가 많아질수록 특정 훈련 데이터에 더 최적화 시킬수가 있다. 또한 모든 hidden layer의 뉴런의 개수를 동일하게 유지하는 것이 같은 hidden layer의 개수에 뉴런의 개수를 가변적으로 하는 것보다 효과적이다. 또한 첫번째 hidden layer에 있는 뉴런의 개수가 input layer에 있는 뉴런의 개수보다 큰것이 효과적인 경우가 많다.\n",
        "\n",
        "7. **가중치 초기화(Weight initialization)**\n",
        "바이어스는 일반적으로 0으로 초기화가 만히 된다. 하지만 가중치의 경우는 초기화가 하습결과에 큰 영향을 미치기 때문에 주의가 필요하다. 가중치는 보통 [-r, r]의 범위를 가진다. 이때 r은 input layer에 있는 뉴런의 개수 제곱의 역수가 된다. 가령 뉴런의 개수가 6이라면, [-1/36, 1/36] 범위 내에서 무작위로 설정을 한다."
      ]
    }
  ]
}